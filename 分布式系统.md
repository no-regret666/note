# 分布式系统

## 分布式协议

### `paxos` 协议

#### `Paxos `的基本思想

`Paxos`模拟了一个分布式环境中如何“安全地做决策”，即使部分节点宕机或网络丢包，仍然保证整个系统不会产生矛盾的决定。

它把一致性过程拆成三个角色：

- Proposer（提议者）：提出一个值，希望被接受。
- Acceptor（接受者）：负责表决是否接受一个提议。
- Learner（学习者）：得知最终选定的值。

只要 多数 Acceptors 接受了同一个提议，这个值就被认为是“达成共识”的。

#### `Paxos `协议流程简述

1. 准备阶段（Prepare）

   - Proposer 生成一个唯一编号 n，向所有 Acceptors 发送 Prepare(n) 请求。

   - 每个 Acceptor：
     - 若 n > 自己记录的最大编号，承诺不再接受编号 < n 的提议；
     - 回复其曾接受过的编号最大的提议（若有）。

2. 提议阶段（Accept）
   - 如果 Proposer 收到多数 Acceptor 的响应：
     - 若没有 Acceptor 曾接受过提议，Proposer 可使用自己的值；
     - 若有，Proposer 必须使用那个最大编号提案的值。
   - 然后发送 Accept(n, v) 请求给所有 Acceptors。

3. 接受阶段
   - 每个 Acceptor 若未违背承诺（即 n ≥ 之前承诺的编号），则接受这个值 v。
   - 一旦多数 Acceptors 接受了某个值，该值就被“选定”。

4. 学习阶段
   - 一旦选定值，Learners 被通知这个值，表示达成共识。

#### `Paxos` 的核心特性

- 安全性（一致性）：不会出现两个不同的值被多数接受。
- 容错性：即使部分节点宕机，只要多数节点正常，就能达成一致。
- 活性：只要有一个 Proposer 能持续和多数 Acceptor 通信，系统最终能达成一致。

####  `Paxos` 的问题

- 实现复杂，逻辑抽象难理解。
- 需要多轮通信，效率不高。
- 在竞争激烈时可能发生“活锁”（多个 Proposer 抢提议）。



### `ZAB`协议

`ZAB` 协议（Zookeeper Atomic Broadcast）是 ZooKeeper 使用的一种 分布式一致性协议，其核心目标是：

在主备架构下，确保分布式系统中的所有节点对数据更新操作顺序一致，即强一致性。

#### ZAB 协议的四个核心阶段

1. 选主（Leader Election）
  - 所有节点开始互相投票选出 Leader（一般使用 ZXID 最大的节点）
  - 选举成功后，Follower 同步数据，准备接受 Leader 广播的事务

2. 发现（Discovery）
  - Leader 收集所有 Follower 的历史 ZXID
  - 确定一组完整一致的数据快照，防止历史事务丢失或重复

3. 同步（Synchronization）
  - Leader 将缺失的数据补发给 Follower
  - 所有 Follower 与 Leader 的状态同步后，进入 广播阶段

4. 广播（Broadcasting / Broadcasting Phase）
  - Leader 接收客户端写请求，生成事务 Proposal
  - 广播 Proposal 给所有 Follower
  - 超过半数 Follower ack 之后，Leader 提交事务
  - 然后通知所有 Follower 提交



### `Raft`协议

Raft 协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。

Leader：所有请求的处理者，Leader 副本接受 client 的更新请求，本地处理后再同步至多个其他副本 Follower：请求的被动更新者，从 Leader 接受更新请求，然后写入本地日志文件 Candidate：如果 Follower 副本在一段时间内没有收到 Leader 副本的心跳，则判断 Leader 可能已经故障，此时启动选主过程，此时副本会变成 Candidate 状态，直到选主结束。

可以看出，跟 Paxos 的基本理念一样，首先最基本的 Flollow（接受者），如果 Follower 接收不到 Leader 的心跳，就会全部转化为Candidate（提议者），然后提议者开始“贿选”，直到选出 Leader 之后，所有没选上的 Candidate 退回到 Follower 状态，统一接收 Leader 领导。

就是说只要 Leader 不挂掉，只要选举一次就行了，后面大家默认信任选出来的 Leader。

另外，每一个副本都会维护一个 term，类似于一个逻辑时钟，每发生一个动作就会递增，通过比较每个提议的 term，副本会默认使用最新的 term，防止发生冲突。如果一个 Leader 或者 Candidate 发现自己的 term 不是最新的了，就会自动降级到 Follower，而如果一个 Follower 接收到低于自己当前 term 的提议，就会直接抛弃。

基本原则了解之后，我们完善一下细节。

在强 Leader 的帮助下，Raft 将一致性问题分解为了三个子问题：

Leader 选举：当已有的 Leader 故障时必须选出一个新的 Leader。
日志复制：Leader 接受来自客户端的命令，记录为日志，并复制给集群中的其他服务器，并强制其他节点的日志与 Leader 保持一致。
安全 safety 措施：通过一些措施确保系统的安全性，如确保所有状态机按照相同顺序执行相同命令的措施。
另外丢两个非常清晰的 Raft 全流程动画演示，看完之后很容易理解：

#### 选举过程

我们从最初始的状态来模拟，假设一个集群有三个副本，刚启动的时候，大家都是 Follower。然后每个 Follower 会有一个倒计时（election timeout），在倒计时结束之前，如果没有收到任何 Leader 的心跳，或者其他 Candidate 的投票请求，就会转化为 Candidate，开始选举。

变成 Candidate 之后，会先投自己一票，同时开启一个倒计时，然后向所有其他节点发起投票请求。如果在倒计时完成之前，没有成为 Leader 或者接收到其他 Leader 的消息，就会发起新一轮选举。

当一个副本处于 Candidate 状态时，如果收到来自 Leader 的心跳消息，就会立即变身为 Follower。如果发出去的投票请求得到了半数节点的成功回应，就会立即变身为 Leader，并周期性地向其它节点广播心跳消息，以尽可能长期维持自己的统治地位。

关于选举的更多细节：

election timeout 会是一个一定范围内的随机值，因为如果所有节点的倒计时时间都一样，大家就会同时变成 Candidate，然后同时互相投票选举，加大了达成共识的难度，所以倒计时会稍微错开，就很容易率先选出来一个 Leader。
成功选举 Leader 之后，Leader 会向所有节点发送心跳，然后心跳会重置每个节点的 election timeout 倒计时时间。
即便错开了倒计时，仍然有可能出现多个 Candidate 同时竞争，如果两个 Candidate 获得的票数不一致还好说，其中一个必然是多数，变成了 Leader。但是如果恰巧节点总数是偶数，就有可能出现票数一样僵持的情况。这时候就会重新选举。（这里我觉得每个 Candidate 发一个随机数过去，谁更大听谁的也行...，当然即便这样也有可能一样大）

#### 数据同步

节点选出来了，下面就应该进行数据同步了。当一个数据修改的请求过来，会直接找到 Leader 节点，所有的增删改查都由 Leader 受理。然后同步给各个 Follower。

每次数据同步操作同时也是一个心跳，会更新 Follower 的 election timeout。另外只有当多数节点返回同步成功之后，Leader 才会给客户端返回操作成功。

#### 分区容错

然后是最麻烦的部分，如果出现了网络分区怎么办？比如原本五个节点的集群，被分成了双节点和三节点的两个集群。

假设原本的 Leader 在双节点的集群里面，那么这个集群会照常运作。而新出现的三个节点的集群，由于没有收到心跳，会开始选举，然后选出新的 Leader。这时候，如果有客户端发起请求，有可能发送到两个不同的 Leader 上面，如果发送到原来的那个 Leader 上，即双节点的集群中，Leader 把操作同步给 Follower，会发现收不到足够多的 Follower 响应（因为这个 Follower 还以为自己的集群是五个节点），然后就没办法同步数据。而三节点的新集群，就可以顺利更新数据。

如果这时候网络恢复了，各个节点又可以正常通信，三节点集群中的 Leader 和 双节点集群中的 Leader 会互相通信，然后会发现三节点的 Leader 由于一直正常运行，term 值会不断增大，所以大家会采信他的数据。于是双节点的两台机器会回滚，然后全部接受新 Leader 的数据同步。

### Raft和ZAB共同点和区别

首先，二者都是通过选举一个 Leader 来简化复杂度，后续的工作都是由 Leader 来做。

投票的时候，二者都需要定义一个轮次

- Raft 定义了 term 来表示选举轮次

- ZooKeeper 定义了 electionEpoch 来表示



同步数据的时候，都希望选举出来的 Leader 至少包含之前全部已提交的日志。

那如何能包含之前的全部日志？我们可以通过判断 Leader 节点中日志的逻辑时间序列，包含越新、越多日志的节点，越有可能包含之前全部的已提交日志。对于两种协议：

- Raft：term 大的优先，然后 entry 的 index 大的优先
- ZooKeeper：peerEpoch 大的优先，然后 zxid 大的优先

ZooKeeper 有 2 个轮次，一个是选举轮次 electionEpoch，另一个是日志的轮次 peerEpoch（即表示这个日志是哪个轮次产生的）。而 Raft 则是只有一个轮次，相当于日志轮次和选举轮次共用了。

但是有一个问题，日志越新越大的比较方式能满足我们“Leader 至少包含之前全部已提交的日志”的愿望吗？

对于 Raft 协议，特殊情况下不能。对于 Raft 协议，通过两个约束来保证一致性：

> 当前 term 的 Leader 不能“直接”提交之前 term 的 entries。
> 必须要等到当前 term 有 entry 过半了，才顺便一起将之前 term 的 entries 进行提交。

但是对于 ZooKeeper 是不会出现这种情况的，因为 ZooKeeper 在每次 Leader 选举完成之后，都会进行数据之间的同步纠正，所以每一个轮次，大家都日志内容都是统一的。



继续对比，二者的选举效率也不同：

- Raft 中的每个节点在某个 term 轮次内只能投一次票，哪个 Candidate 先请求投票谁就可能先获得投票，这样就可能造成分区，即各个 Candidate 都没有收到过半的投票，Raft 通过 Candidate 设置不同的超时时间，来快速解决这个问题，使得先超时的Candidate（在其他人还未超时时）优先请求来获得过半投票。
- ZooKeeper 中的每个节点，在某个 electionEpoch 轮次内，可以投多次票，只要遇到更大的票就更新，然后分发新的投票给所有人。这种情况下不存在分区现象，同时有利于选出含有更新更多的日志的 Server，但是选举时间理论上相对 Raft 要花费的多。

在一个节点启动后，如何加入一个集群（这里是说本来就在集群配置内的一个节点）：

- Raft：比较简单，该节点启动后，会收到 Leader 的 AppendEntries RPC，在这个 RPC 里面包含 Leader 信息，可以直接识别。
- ZooKeeper：启动后，会向所有的其他节点发送投票通知，然后收到其他节点的投票。该节点只需要判断上述投票是否过半，过半则可以确认 Leader。



关于 Leader 选举的触发：

首先集群启动的时候，二者肯定都要先进行选举。

如果选举完成后，发生了超时：

- Raft：目前只是 Follower 在检测。如过 Follower 在倒计时时间内未收到 Leader 的心跳信息，则 Follower 转变成 Candidate，自增 term 发起新一轮的投票。

- ZooKeeper：Leader 和 Follower 都有各自的检测超时方式，Leader 是检测是否过半 Follower 心跳回复了，Follower 检测 Leader 是否发送心跳了。一旦 Leader 检测失败，则 Leader 进入 Looking 状态，其他 Follower 过一段时间因收不到 Leader 心跳也会进入 Looking 状态，从而出发新的 Leader 选举。一旦 Follower 检测失败了，则该 Follower 进入 Looking 状态，此时 Leader 和其他 Follower 仍然保持良好，则该 Follower 仍然是去学习上述 Leader 的投票，而不是触发新一轮的 Leader 选举。

  



关于上一轮次 Leader 残存的数据怎么处理：

包括两种数据：

- 已过半复制的日志

- 未过半复制的日志
- Raft：对于之前 term 的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当前 term 的日志过半了，才会顺便将之前 term 的日志进行提交
- ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中



Raft 的保守策略更多是因为 Raft 在 Leader 选举完成之后，没有同步更新过程来保持和 Leader 一致（在可以对外处理请求之前的这一同步过程）。而 ZooKeeper 是有该过程的。

在对正常请求的处理方式上，二者都是基本相同的，大致过程都是过半复制。

对于正常请求的消息顺序保证：

- Raft：对请求先转换成 entry，复制时，也是按照 Leader 中 log 的顺序复制给 Follower 的，对 entry 的提交是按 index 进行顺序提交的，是可以保证顺序的

- ZooKeeper：在提交议案的时候也是按顺序写入各个 Follower 对应在 Leader 中的队列，然后 Follower 必然是按照顺序来接收到议案的，对于议案的过半提交也都是一个个来进行的



如果是 Leader 挂了之后，重新选举出 Leader，会不会有乱序的问题？

- Raft：Raft 对于之前 term 的 entry 被过半复制暂不提交，只有当本 term 的数据提交了才能将之前 term 的数据一起提交，也是能保证顺序的
- ZooKeeper：ZooKeepe r每次 Leader 选举之后都会进行数据同步，不会有乱序问题



在出现网络分区情况下的应对措施，二者都是相同的：

目前 ZooKeeper 和 Raft 都是过半即可，所以对于分区是容忍的。如5台机器，分区发生后分成 2 部分，一部分 3 台，另一部分 2 台，这 2 部分之间无法相互通信。

其中，含有 3 台的那部分，仍然可以凑成一个过半，仍然可以对外提供服务，但是它不允许有节点再挂了，一旦再挂一台则就全部不可用了。

含有 2 台的那部分，则无法提供服务，即只要连接的是这 2 台机器，都无法执行相关请求。

所以 ZooKeeper 和 Raft 在一旦分区发生的情况下是是牺牲了高可用来保证一致性，即 CAP 理论中的 CP，二者都是 CP 系统。





<img src="/home/noregret/.config/Typora/typora-user-images/image-20250730214756038.png" alt="image-20250730214756038" style="zoom: 67%;" />



## 分布式事务

### `XA`两阶段提交协议

强一致性设计，引入一个事务协调者的角色来协调管理各参与者的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。

1. 准备阶段

   在此阶段，协调者询问所有参与者是否可以提交事务。如果参与者的事务操作实际执行成功，则返回一个“同意”消息；如果执行失败，则返回一个“终止”消息。

   <img src="https://img2024.cnblogs.com/blog/167509/202403/167509-20240325210028598-392238423.png" alt="image" style="zoom:33%;" />

​	准备阶段只要有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。如下图:

2. 提交阶段

   协调者根据所有参与者的应答结果判定是否事务可以全局提交（Commit 请求），并通知所有参与者执行该决定。

   如果所有参与者都同意提交，则协调者让所有参与者都提交事务，向事务协调者返回“完成”消息。整个分布式事务完成。
   如果其中某个参与者终止提交，则协调者让所有参与者都回滚事务。

   <img src="https://img2024.cnblogs.com/blog/167509/202403/167509-20240325210117434-1593593393.png" alt="image" style="zoom: 33%;" />

​	如果其中一个Commit 不成功，那其他的应该也是提交不成功的。

<img src="https://img2024.cnblogs.com/blog/167509/202403/167509-20240325210135020-1133741005.png" alt="image" style="zoom:33%;" />

可能会存在哪些问题？
「单点故障」：一旦事务管理器出现故障，整个系统不可用
「数据不一致」：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。
「响应时间较长」：整个消息链路是串行的，要等待响应结果，不适合高并发的场景
「不确定性」：当事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。



### `XA`三阶段提交

三阶段提交：CanCommit 阶段、PreCommit 阶段、DoCommit 阶段，简称3PC。

三阶段提交协议（Three-phase commit protocol，3PC），是二阶段提交（2PC）的改进版本。与两阶段提交不同的是，三阶段提交有两个改动点：在协调者和参与者中都引入超时机制，同时引入了预提交阶段。

在第一阶段和第二阶段中插入的预提交阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。
即 3PC 把 2PC 的准备阶段再次一分为二，这样三阶段提交就有 CanCommit、PreCommit、DoCommit 三个阶段。当 CanCommit、PreCommit、DoCommit的任意一个步骤失败或者等待超时，执行RollBack。

<img src="https://img2024.cnblogs.com/blog/167509/202403/167509-20240325210231435-1830002145.png" alt="image" style="zoom: 50%;" />

通过引入PreCommit阶段，3PC在一定程度上解决了2PC中协调者单点故障的问题，因为即使协调者在PreCommit阶段后发生故障，参与者也可以根据自身的状态来决定是否提交事务。然而，3PC并不是完美的解决方案，它仍然有一些缺点，比如增加了协议的复杂性和可能的性能开销。因此，在选择是否使用3PC时，需要根据具体的业务场景和需求进行权衡。

### `MQ`事务

利用消息中间件来异步完成事务的后半部分更新，实现系统的最终一致性。 这个方式避免了像XA协议那样的性能问题。
下面的图中，使用MQ完成事务在分布式的另外一个子系统上的操作，保证了动作一致性。所以整个消息的生产和消息的消费动作需要全部完成，才算一个事务结束

<img src="https://img2024.cnblogs.com/blog/167509/202403/167509-20240325210300125-725311391.png" alt="image" style="zoom:50%;" />

### `TCC`事务

`TCC`其实就是采用的补偿机制，其核心思想是：**「针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作」**。它分为三个阶段：

**「Try,Confirm,Cancel」**

- Try阶段主要是对「**业务系统做检测及资源预留**」，其主要分为两个阶段
  - Confirm 阶段主要是对**「业务系统做确认提交」**，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。
  - Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，**「预留资源释放」**。

比如下一个订单减一个库存：

<img src="https://ask.qcloudimg.com/http-save/8352478/s6k4jk82zh.png" alt="img" style="zoom:50%;" />

执行流程：

- Try阶段：订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于1，然后将可用库存数量设置为库存剩余数量-1，
  - 如果Try阶段**「执行成功」**，执行Confirm阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量
  - 如果Try阶段**「执行失败」**，执行Cancel阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量

TCC 事务机制相比于上面介绍的2PC，解决了其几个缺点：

- 1.**「解决了协调者单点」**，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。
- 2.**「同步阻塞」**：引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。
- 3.**「数据一致性」**，有了补偿机制之后，由业务活动管理器控制一致性

总之，TCC 就是通过代码人为实现了两阶段提交，不同的业务场景所写的代码都不一样，并且很大程度的**「增加」**了业务代码的**「复杂度」**，因此，这种模式并不能很好地被复用。



### `Saga` 事务

Saga是由一系列的本地事务构成。每一个本地事务在更新完数据库之后，会发布一条消息或者一个事件来触发Saga中的下一个本地事务的执行。如果一个本地事务因为某些业务规则无法满足而失败，Saga会执行在这个失败的事务之前成功提交的所有事务的补偿操作。

Saga的实现有很多种方式，其中最流行的两种方式是：

- **基于事件的方式**。这种方式没有协调中心，整个模式的工作方式就像舞蹈一样，各个舞蹈演员按照预先编排的动作和走位各自表演，最终形成一只舞蹈。处于当前Saga下的各个服务，会产生某类事件，或者监听其它服务产生的事件并决定是否需要针对监听到的事件做出响应。
- **基于命令的方式**。这种方式的工作形式就像一只乐队，由一个指挥家（协调中心）来协调大家的工作。协调中心来告诉Saga的参与方应该执行哪一个本地事务。

我们继续以订单流程为例，说明一下该模式。

假设一个完整的订单流程包含了如下几个服务：

1. Order Service：订单服务
2. Payment Service：支付服务
3. Stock Service：库存服务
4. Delivery Service：物流服务

<img src="https://pdai.tech/images/arch/arch-transection-31.webp" alt="img" style="zoom:50%;" />

#### 基于事件的方式

在基于事件的方式中，第一个服务执行完本地事务之后，会产生一个事件。其它服务会监听这个事件，触发该服务本地事务的执行，并产生新的事件。

采用基于事件的saga模式的订单处理流程如下：

<img src="https://pdai.tech/images/arch/arch-transection-32.webp" alt="img" style="zoom:50%;" />

1. 订单服务创建一笔新订单，将订单状态设置为"待处理"，产生事件ORDER_CREATED_EVENT。
2. 支付服务监听ORDER_CREATED_EVENT，完成扣款并产生事件BILLED_ORDER_EVENT。
3. 库存服务监听BILLED_ORDER_EVENT，完成库存扣减和备货，产生事件ORDER_PREPARED_EVENT。
4. 物流服务监听ORDER_PREPARED_EVENT，完成商品配送，产生事件ORDER_DELIVERED_EVENT。
5. 订单服务监听ORDER_DELIVERED_EVENT，将订单状态更新为"完成"。

在这个流程中，订单服务很可能还会监听BILLED_ORDER_EVENT，ORDER_PREPARED_EVENT来完成订单状态的实时更新。将订单状态分别更新为"已经支付"和"已经出库"等状态来及时反映订单的最新状态。

##### 该模式下分布式事务的回滚

为了在异常情况下回滚整个分布式事务，我们需要为相关服务提供补偿操作接口。

假设库存服务由于库存不足没能正确完成备货，我们可以按照下面的流程来回滚整个Saga事务：

<img src="https://pdai.tech/images/arch/arch-transection-33.webp" alt="img" style="zoom:50%;" />

1. 库存服务产生事件PRODUCT_OUT_OF_STOCK_EVENT。
2. 订单服务和支付服务都会监听该事件并做出响应：
   1. 支付服务完成退款。
   2. 订单服务将订单状态设置为"失败"。

##### 基于事件方式的优缺点

优点：简单且容易理解。各参与方相互之间无直接沟通，完全解耦。这种方式比较适合整个分布式事务只有2-4个步骤的情形。

缺点：这种方式如果涉及比较多的业务参与方，则比较容易失控。各业务参与方可随意监听对方的消息，以至于最后没人知道到底有哪些系统在监听哪些消息。更悲催的是，这个模式还可能产生环形监听，也就是两个业务方相互监听对方所产生的事件。

接下来，我们将介绍如何使用命令的方式来克服上面提到的缺点



#### 基于命令的方式

在基于命令的方式中，我们会定义一个新的服务，这个服务扮演的角色就和一支交响乐乐队的指挥一样，告诉各个业务参与方，在什么时候做什么事情。我们管这个新服务叫做协调中心。协调中心通过命令/回复的方式来和Saga中其它服务进行交互。

我们继续以之前的订单流程来举例。下图中的Order Saga Orchestrator就是新引入的协调中心。

<img src="https://pdai.tech/images/arch/arch-transection-34.webp" alt="img" style="zoom:50%;" />

1. 订单服务创建一笔新订单，将订单状态设置为"待处理"，然后让Order Saga Orchestrator（OSO）开启创建订单事务。
2. OSO发送一个"支付命令"给支付服务，支付服务完成扣款并回复"支付完成"消息。
3. OSO发送一个"备货命令"给库存服务，库存服务完成库存扣减和备货，并回复"出库"消息。
4. OSO发送一个"配送命令"给物流服务，物流服务完成配送，并回复"配送完成"消息。
5. OSO向订单服务发送"订单结束命令"给订单服务，订单服务将订单状态设置为"完成"。
6. OSO清楚一个订单处理Saga的具体流程，并在出现异常时向相关服务发送补偿命令来回滚整个分布式事务。

实现协调中心的一个比较好的方式是使用**状态机(Sate Machine)**。

##### 该模式下分布式事务的回滚

该模式下的回滚流程如下：

<img src="https://pdai.tech/images/arch/arch-transection-35.webp" alt="img" style="zoom:50%;" />

库存服务回复OSO一个"库存不足"消息。

OSO意识到该分布式事务失败了，触发回滚流程：

OSO发送"退款命令"给支付服务，支付服务完成退款并回复"退款成功"消息。

OSO向订单服务发送"将订单状态改为失败命令"，订单服务将订单状态更新为"失败"。

##### 基于命令方式的优缺点

优点：

避免了业务方之间的环形依赖。
将分布式事务的管理交由协调中心管理，协调中心对整个逻辑非常清楚。
减少了业务参与方的复杂度。这些业务参与方不再需要监听不同的消息，只是需要响应命令并回复消息。
测试更容易（分布式事务逻辑存在于协调中心，而不是分散在各业务方）。
回滚也更容易。

缺点：

一个可能的缺点就是需要维护协调中心，而这个协调中心并不属于任何业务方。
